{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9GCGMtwyohS"
   },
   "source": [
    "# Convolutional Neural Networks (CNN)\n",
    "\n",
    "In the first part of this lab session we will explore how __convolutions__ work, basic __CNN architectures__ and __impact of hyperparameters__. <br>\n",
    "In the second part you will integrate neural network trained in part 1, to the drawing program to continuously predict drawn object.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRp_qWdxyohW"
   },
   "source": [
    "## Basic Imports\n",
    "\n",
    "For this lab session, we will be needing Keras, Numpy and Tensorflow. <br>\n",
    "We will build our CNN in Keras, but first we need to understand the underlying principles, for which we will use Tensorflow. <br>\n",
    "Numpy will be mainly used for dataset preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5P4TN6uLyohX"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "from numpy.random import seed\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=2\n",
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:0b:00.0, compute capability: 7.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    %env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "    %env CUDA_VISIBLE_DEVICES=2\n",
    "except NameError:\n",
    "    os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "## ALlowing growth for remote server GPUs\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "tf.compat.v1.keras.backend.set_session(sess)  # set this TensorFlow session as the default session for Keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coF3bUaByohe"
   },
   "source": [
    "## Replicability\n",
    "While experimenting and researching, it is important that your results can be __replicated by other people__. <br>\n",
    "To ensure some level of replicability, we can __set the starting seed__ of both numpy and tensorflow __to known value__. <br>\n",
    "Therefore, when we initialise our network to random values, these states can be calculated and replicated just by knowing the seed.<br>\n",
    "Sadly, due to the lossy nature of GPU calculations, training itself cannot be perfectly replicated, but it is still good practice to set initial seeds to known values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "68_hPQu8yohf"
   },
   "outputs": [],
   "source": [
    "seed(101)\n",
    "tf.random.set_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VE6znDdhyohj"
   },
   "source": [
    "## Low Level Code\n",
    "\n",
    "While building model, we will add whole convolution layer in a neat package `Conv2D()`. <br>\n",
    "But before we do so, it is good to understand the __underlaying mechanics__ and __code implementation__ of convolutions. <br>\n",
    "In the following example, we will define our image/data array `inputs`, and using `kernel` apply 2D convolution. <br>\n",
    "Full implemenation of convolution layer can be found <a src=\"https://github.com/keras-team/keras/blob/8ed57c168f171de7420e9a96f9e305b8236757df/keras/layers/convolutional.py#L161\"> here</a>. <br>\n",
    "\n",
    "\n",
    "A convolution input must have shape of `(BatchSize, width, height, inputChannels)` <br>\n",
    "A convolution filter must have shape of `(width, height, inputChannels, outputChannels)` <br>\n",
    "\n",
    "### Task\n",
    "1) Experiment with different strides, kernel, padding etc. <br>\n",
    "2) Why does first ouput element equal to 21.0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMy6HP4Wyohi"
   },
   "source": [
    "# Convolutions\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are designed to learn features directly from image pixels. They can classify patterns or objects with extreme variability. Currently, they form the core of various __computer vision systems__ such as Facebooks automated photo tagging, handwritten characters recognition, self-driving cars, marine mammal detection, and medical image analysis. In this lab, we will start by exploring a convolution function which forms the heart of CNNs.\n",
    "\n",
    "![Convolution](https://raw.githubusercontent.com/RetinalSW/COM3025/master/data/convolution_kernal.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EDUa_aP1yohj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of an Input: <bound method _EagerTensorBase.get_shape of <tf.Tensor: shape=(1, 5, 5, 1), dtype=float32, numpy=\n",
      "array([[[[ 1.],\n",
      "         [ 2.],\n",
      "         [ 3.],\n",
      "         [ 4.],\n",
      "         [ 5.]],\n",
      "\n",
      "        [[ 6.],\n",
      "         [ 7.],\n",
      "         [ 8.],\n",
      "         [ 9.],\n",
      "         [10.]],\n",
      "\n",
      "        [[11.],\n",
      "         [12.],\n",
      "         [13.],\n",
      "         [14.],\n",
      "         [15.]],\n",
      "\n",
      "        [[16.],\n",
      "         [17.],\n",
      "         [18.],\n",
      "         [19.],\n",
      "         [20.]],\n",
      "\n",
      "        [[21.],\n",
      "         [22.],\n",
      "         [23.],\n",
      "         [24.],\n",
      "         [25.]]]], dtype=float32)>>\n",
      "Shape of a Kernel <bound method _EagerTensorBase.get_shape of <tf.Tensor: shape=(3, 3, 1, 1), dtype=float32, numpy=\n",
      "array([[[[1.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[0.]]],\n",
      "\n",
      "\n",
      "       [[[0.]],\n",
      "\n",
      "        [[1.]],\n",
      "\n",
      "        [[0.]]],\n",
      "\n",
      "\n",
      "       [[[0.]],\n",
      "\n",
      "        [[0.]],\n",
      "\n",
      "        [[1.]]]], dtype=float32)>>\n",
      "Shape of result: <bound method _EagerTensorBase.get_shape of <tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=\n",
      "array([[[[21.],\n",
      "         [24.],\n",
      "         [27.]],\n",
      "\n",
      "        [[36.],\n",
      "         [39.],\n",
      "         [42.]],\n",
      "\n",
      "        [[51.],\n",
      "         [54.],\n",
      "         [57.]]]], dtype=float32)>>\n",
      "Result: [[[[21.]\n",
      "   [24.]\n",
      "   [27.]]\n",
      "\n",
      "  [[36.]\n",
      "   [39.]\n",
      "   [42.]]\n",
      "\n",
      "  [[51.]\n",
      "   [54.]\n",
      "   [57.]]]]\n"
     ]
    }
   ],
   "source": [
    "# We need keras.backend and tensorflow to create proper tensors directly\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "inputs = tf.constant([[[1.0],  [2.0],  [3.0],  [4.0],  [5.0]],\n",
    "                      [[6.0],  [7.0],  [8.0],  [9.0],  [10.0]],\n",
    "                      [[11.0], [12.0], [13.0], [14.0], [15.0]],\n",
    "                      [[16.0], [17.0], [18.0], [19.0], [20.0]],\n",
    "                      [[21.0], [22.0], [23.0], [24.0], [25.0]]\n",
    "                     ])\n",
    "\n",
    "\n",
    "kernel = tf.constant([[1.0,0.0,0.0],\n",
    "                      [0.0,1.0,0.0],\n",
    "                      [0.0,0.0,1.0]])\n",
    "\n",
    "\n",
    "\n",
    "inputs = K.reshape(inputs,(-1,5,5,1))\n",
    "print(\"Shape of an Input:\", inputs.get_shape)\n",
    "\n",
    "kernel =K.reshape(kernel,(3,3,1,1))\n",
    "print(\"Shape of a Kernel\", kernel.get_shape)\n",
    "\n",
    "strides=(1, 1)\n",
    "padding='valid'\n",
    "\n",
    "result = K.conv2d(inputs, kernel, strides=strides, padding=padding)\n",
    "print(\"Shape of result:\", result.get_shape)\n",
    "\n",
    "print(\"Result:\", K.eval(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjIURbvsyohm"
   },
   "source": [
    "# Dropout\n",
    "Dropout consists of __randomly__ setting a fraction `rate` of input units to 0 at each update __during training time__\n",
    "which helps to __prevent overfitting__. <br>\n",
    "To balance the overall signal strength, we increase the non-zero outputs accordingly. \n",
    "\n",
    "\n",
    "## Arguments\n",
    "`rate`: float between 0 and 1. Fraction of the input units to drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "4pPZrNnNyohm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: [0. 5. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.constant([1.0, 1.0, 1.0, 1.0, 1.0, 1.0])\n",
    "rate = 0.8\n",
    "\n",
    "result = K.dropout(inputs, rate, None)\n",
    "print(\"Result:\", K.eval(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12ATZEXIyoho"
   },
   "source": [
    "# Pooling\n",
    "\n",
    "## MaxPooling\n",
    "\n",
    "Another important concept of CNNs is max-pooling, which is a form of\n",
    "non-linear down-sampling. Max-pooling partitions the input image into a\n",
    "set of non-overlapping rectangles and, for each such sub-region, outputs\n",
    "the maximum value.\n",
    "Max-pooling is useful in vision for two reasons:\n",
    "- By eliminating non-maximal values, it reduces computation for upper layers.\n",
    "- It provides a form of translation invariance.\n",
    "\n",
    "\n",
    "## AveragePooling\n",
    "Alternative to MaxPooling is Average pooling, where you take sum of all elements in pool and divide by number of elements. \n",
    "\n",
    "Experiment with different strides, kernel, padding etc. <br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "520X_-ziyoho"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of an Input: <bound method _EagerTensorBase.get_shape of <tf.Tensor: shape=(1, 5, 5, 1), dtype=float32, numpy=\n",
      "array([[[[ 1.],\n",
      "         [ 2.],\n",
      "         [ 3.],\n",
      "         [ 4.],\n",
      "         [ 5.]],\n",
      "\n",
      "        [[ 6.],\n",
      "         [ 7.],\n",
      "         [ 8.],\n",
      "         [ 9.],\n",
      "         [10.]],\n",
      "\n",
      "        [[11.],\n",
      "         [12.],\n",
      "         [13.],\n",
      "         [14.],\n",
      "         [15.]],\n",
      "\n",
      "        [[16.],\n",
      "         [17.],\n",
      "         [18.],\n",
      "         [19.],\n",
      "         [20.]],\n",
      "\n",
      "        [[21.],\n",
      "         [22.],\n",
      "         [23.],\n",
      "         [24.],\n",
      "         [25.]]]], dtype=float32)>>\n",
      "<bound method _EagerTensorBase.get_shape of <tf.Tensor: shape=(1, 4, 4, 1), dtype=float32, numpy=\n",
      "array([[[[ 7.],\n",
      "         [ 8.],\n",
      "         [ 9.],\n",
      "         [10.]],\n",
      "\n",
      "        [[12.],\n",
      "         [13.],\n",
      "         [14.],\n",
      "         [15.]],\n",
      "\n",
      "        [[17.],\n",
      "         [18.],\n",
      "         [19.],\n",
      "         [20.]],\n",
      "\n",
      "        [[22.],\n",
      "         [23.],\n",
      "         [24.],\n",
      "         [25.]]]], dtype=float32)>>\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.constant([[[1.0],  [2.0],  [3.0],  [4.0],  [5.0]],\n",
    "                      [[6.0],  [7.0],  [8.0],  [9.0],  [10.0]],\n",
    "                      [[11.0], [12.0], [13.0], [14.0], [15.0]],\n",
    "                      [[16.0], [17.0], [18.0], [19.0], [20.0]],\n",
    "                      [[21.0], [22.0], [23.0], [24.0], [25.0]]\n",
    "                     ])\n",
    "\n",
    "inputs = K.reshape(inputs,(-1,5,5,1))\n",
    "print(\"Shape of an Input:\", inputs.get_shape)\n",
    "\n",
    "pool_size = (2,2)\n",
    "strides=(1, 1)\n",
    "padding='valid'\n",
    "data_format=None\n",
    "pool_mode='max' # or use 'avg'\n",
    "\n",
    "result = K.pool2d(inputs, pool_size=pool_size, strides = strides,\n",
    "                          padding = padding, data_format = data_format,\n",
    "                          pool_mode=pool_mode)\n",
    "\n",
    "print(result.get_shape)\n",
    "# print(\"Result:\", K.eval(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PZ0c4OppRLv"
   },
   "source": [
    "# Stride and Padding\n",
    "- You might have noticed the padding and stride, but do you know what the stride and padding exactly are?\n",
    "## Convolution animations\n",
    "\n",
    "_N.B.: Blue maps are inputs, and cyan maps are outputs._\n",
    "\n",
    "<table style=\"width:100%; table-layout:fixed;\">\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/arbitrary_padding_no_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/same_padding_no_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/full_padding_no_strides.gif\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, no strides</td>\n",
    "    <td>Arbitrary padding, no strides</td>\n",
    "    <td>Half padding, no strides</td>\n",
    "    <td>Full padding, no strides</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides_odd.gif\"></td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, strides</td>\n",
    "    <td>Padding, strides</td>\n",
    "    <td>Padding, strides (odd)</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "## Transposed convolution animations\n",
    "\n",
    "_N.B.: Blue maps are inputs, and cyan maps are outputs._\n",
    "\n",
    "<table style=\"width:100%; table-layout:fixed;\">\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_no_strides_transposed.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/arbitrary_padding_no_strides_transposed.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/same_padding_no_strides_transposed.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/full_padding_no_strides_transposed.gif\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, no strides, transposed</td>\n",
    "    <td>Arbitrary padding, no strides, transposed</td>\n",
    "    <td>Half padding, no strides, transposed</td>\n",
    "    <td>Full padding, no strides, transposed</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/no_padding_strides_transposed.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides_transposed.gif\"></td>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/padding_strides_odd_transposed.gif\"></td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, strides, transposed</td>\n",
    "    <td>Padding, strides, transposed</td>\n",
    "    <td>Padding, strides, transposed (odd)</td>\n",
    "    <td></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "## Dilated convolution animations\n",
    "\n",
    "_N.B.: Blue maps are inputs, and cyan maps are outputs._\n",
    "\n",
    "<table style=\"width:25%\"; table-layout:fixed;>\n",
    "  <tr>\n",
    "    <td><img width=\"150px\" src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/dilation.gif\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No padding, no stride, dilation</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "### Reference: [https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mohrx8gns73e"
   },
   "source": [
    "## <font color='red'>Task</font>\n",
    "We have a input of (1x4x4x1)\n",
    "- Can you output make a Conv2D that give the output that is the same size as the input?\n",
    "- Can you make the output a half smaller than the original output? `(1x2x2x1)`\n",
    "- You can verify by printing out the result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Cf7qUPM0s3zk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of an Input: (1, 4, 4, 1)\n",
      "(1, 2, 2, 1)\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.constant([[[1.0],  [2.0],  [3.0],  [4.0]],\n",
    "                      [[6.0],  [7.0],  [8.0],  [9.0]],\n",
    "                      [[16.0], [17.0], [18.0], [19.0]],\n",
    "                      [[21.0], [22.0], [23.0], [24.0]]\n",
    "                     ])\n",
    "\n",
    "inputs = K.reshape(inputs,(-1,4,4,1))\n",
    "print(\"Shape of an Input:\", inputs.shape)\n",
    "# Your code here (make the same size)\n",
    "strides=(1, 1)\n",
    "padding='same'\n",
    "result = K.conv2d(inputs, kernel, strides=strides, padding=padding)\n",
    "# print(result.shape)\n",
    "\n",
    "# Make sure your output's shape is the same as the input\n",
    "\n",
    "# Your code here (make a half smaller)\n",
    "strides=(1, 1)\n",
    "padding='valid'\n",
    "result = K.conv2d(inputs, kernel, strides=strides, padding=padding)\n",
    "print(result.shape)\n",
    "\n",
    "# Make sure your output's shape is the same as the input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjKP2Gouyohq"
   },
   "source": [
    "# Flatten\n",
    "\n",
    "This function will convert tensor with any shape/number of dimensions to tensor of 1 dimension. <br>\n",
    "Flattening is very commonly used to convert output from convolution layer to dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Qj9NJWp6yohr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of an Input: <bound method _EagerTensorBase.get_shape of <tf.Tensor: shape=(1, 5, 5, 1), dtype=float32, numpy=\n",
      "array([[[[ 1.],\n",
      "         [ 2.],\n",
      "         [ 3.],\n",
      "         [ 4.],\n",
      "         [ 5.]],\n",
      "\n",
      "        [[ 6.],\n",
      "         [ 7.],\n",
      "         [ 8.],\n",
      "         [ 9.],\n",
      "         [10.]],\n",
      "\n",
      "        [[11.],\n",
      "         [12.],\n",
      "         [13.],\n",
      "         [14.],\n",
      "         [15.]],\n",
      "\n",
      "        [[16.],\n",
      "         [17.],\n",
      "         [18.],\n",
      "         [19.],\n",
      "         [20.]],\n",
      "\n",
      "        [[21.],\n",
      "         [22.],\n",
      "         [23.],\n",
      "         [24.],\n",
      "         [25.]]]], dtype=float32)>>\n",
      "[[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
      "  19. 20. 21. 22. 23. 24. 25.]]\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.constant([[[1.0],  [2.0],  [3.0],  [4.0],  [5.0]],\n",
    "                      [[6.0],  [7.0],  [8.0],  [9.0],  [10.0]],\n",
    "                      [[11.0], [12.0], [13.0], [14.0], [15.0]],\n",
    "                      [[16.0], [17.0], [18.0], [19.0], [20.0]],\n",
    "                      [[21.0], [22.0], [23.0], [24.0], [25.0]]\n",
    "                     ])\n",
    "\n",
    "inputs = K.reshape(inputs,(-1,5,5,1))\n",
    "print(\"Shape of an Input:\", inputs.get_shape)\n",
    "\n",
    "\n",
    "result = K.batch_flatten(inputs)\n",
    "print(K.eval(result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeqMaHMPyohs"
   },
   "source": [
    "# Dataset\n",
    "\n",
    "We will walk through building a CNN handwritten digits classifier using the MNIST which is one of the classical datasets for neural networks. <br>\n",
    "\n",
    "We use a pickled version of __MNIST data__ for Python. Use the load method to\n",
    "load the MNIST data. \n",
    "\n",
    "## <font color='red'>Tasks</font>\n",
    "- Please load the mnist data with Keras Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sk-AN_dwyoht"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "# Your code here, replace None with mnist data\n",
    "(X_train_orig, y_train_orig), (X_test_orig, y_test_orig) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4VYVCzZ7yohv"
   },
   "source": [
    "## Visualizing the dataset\n",
    "We will use __matplot library__ to display an image from the MNIST dataset. <br>\n",
    "`%matplotlib inline` will allow us to display this image directly in Jupyter Notebook cell. <br>\n",
    "Since we have __grayscale__ image, we need to specify that, while displaying it via `cmap='gray'`\n",
    "## <font color='red'>Task</font>\n",
    "- Please try to visualize more image data, so you can grasp what is in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "5zStjuHIyohw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f85b4052150>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.imshow(X_train_orig[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QFMVC3S-oyI7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fabf44c9310>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Your code here, try to plot another image, maybe X_train_orig[1] or other image you like\n",
    "plt.imshow(X_train_orig[1523], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Djhb9544ra4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  15  52\n",
      "  148 192 254 253 170   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   8 155 190 191 211 252\n",
      "  252 252 253 252  91   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   2  22 110 145 232 234 252 252 253 245 178\n",
      "  210 252 250 160   2   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 162 252 252 253 252 252 252 155  84  56   6\n",
      "  190 252 211   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  36 241 252 252 191 112  42  42   7   0   0  66\n",
      "  252 252 167   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  36 203 140  18   0   0   0   0   0   0   0 233\n",
      "  253 174   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 100 247\n",
      "  231  28   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  48 247 251\n",
      "   86   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  27 218 252 134\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 106 253 252  21\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  29 219 247  53   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  43 197 252 176   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0 180 252 199   9   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  11 143 246 183  49   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0 175 252 244  49   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0 124 255 239  17   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  84 242 222  42   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  11 211 252  62   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 162 252 182   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0 232 252 103   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Try to print the raw value from data, to see what is it like\n",
    "print(X_train_orig[1523])\n",
    "# Try to print the size a image, so you might know how it looks numerically\n",
    "print(X_train_orig[1523].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rosLmrd4yohx"
   },
   "source": [
    "# Shaping dataset\n",
    "\n",
    "## Images\n",
    "Currently, the X part of dataset is in form `(number_of_samples, px_width, px_height)` <br>\n",
    "There is one implied information about the dataset, but we need to directly specify it. This information is regarding number of channels per image. Since MNIST dataset is only greyscale, we need to specify it in the dimensionality of the dataset.\n",
    "Therefore, we need to convert it from `(60000, 28, 28)` to `(60000,28,28,1)`, where `1` stands for greyscale. <br>\n",
    "If we had an RGB image, the shape of the dataset would look like this `(60000,28,28,3)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cPL8UqHuyohy"
   },
   "outputs": [],
   "source": [
    "X_train = X_train_orig.reshape(60000,28,28,1)\n",
    "X_test = X_test_orig.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xkIR7OByohz"
   },
   "source": [
    "## Labels\n",
    "\n",
    "Label for each image is in form of an __integer__ ranging from 0 to 9. <br>\n",
    "We can use a __one hot encoding__ to transform them into a __binary matrix__. We know there are 10\n",
    "classes for this problem, so we can expect the binary matrix to have a width\n",
    "of 10.\n",
    "\n",
    "### Converting labels to one-hot representation\n",
    "y_train_orig[0] <b>before</b> conversion is <b>[5]</b> <br>\n",
    "y_train_orig[0] <b>after</b> conversion is <b>[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "nfSr-xokyohz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(y_train_orig[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5YpOp9q-yoh1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "to_categorical(y_train_orig[0], 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v_5DZaa7ryVw"
   },
   "source": [
    "## <font color='red'>Task</font>\n",
    "- If we want to convert `y_train_orig` to **20** class representation\n",
    "  - what we need to do?\n",
    "  - What happen if we convert a 10 class label into 20 class one-hot?\n",
    "  - What happen if we convert a 10 class label into 2 class one-hot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "KhQL3Uo4sBH1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert y_train_orig to 20 classes one-hot representation\n",
    "to_categorical(y_train_orig[0], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 1 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-698ce574cbb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_orig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/padchest/lib/python3.7/site-packages/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mcategorical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0moutput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 5 is out of bounds for axis 1 with size 2"
     ]
    }
   ],
   "source": [
    "to_categorical(y_train_orig[0], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgPHFy_eyoh2"
   },
   "source": [
    "### Converting all labels to one-hot matrix\n",
    "We will use the same `to_categorical` function to convert the whole dataset into matrix of one-hot encodings.\n",
    "\n",
    "#### <font color='red'>Task</font>\n",
    "1) What is the shape of newly created dataset? <br>\n",
    "2) [Optional] Instead of pre-made `to_categorical` function, can you code your own with same functionality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_QmtRZvLyoh2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train = to_categorical(y_train_orig)\n",
    "print(y_train.shape)\n",
    "y_test = to_categorical(y_test_orig)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-7-551af0f56730>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-551af0f56730>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    input_shape = tuple(input_shape[:-1])\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "## Custom to_categorical function\n",
    "def to_categorical(y, num_classes=None, dtype='float32'):\n",
    "    y = np.array(y, dtype='int')\n",
    "    input_shape = y.shape\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "    input_shape = tuple(input_shape[:-1])\n",
    "    y = y.ravel()\n",
    "    if not num_classes:\n",
    "    num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes), dtype=dtype)\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijrxqSVzyoh3"
   },
   "source": [
    "# Building the model\n",
    "In this section we will combine previously demonstrated mechanisms into one system. <br>\n",
    "\n",
    "For this very simple model, we will be using `Conv2D`, `Flatten` and `Dense` layers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cIuFd9mTyoh4"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(28,28,1)))\n",
    "    model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wf0bR7ZCyoh5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.2426 - accuracy: 0.9482 - val_loss: 0.0908 - val_accuracy: 0.9716\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 3s 48us/sample - loss: 0.0619 - accuracy: 0.9813 - val_loss: 0.1025 - val_accuracy: 0.9675\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.0422 - accuracy: 0.9868 - val_loss: 0.0848 - val_accuracy: 0.9763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8594360210>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qmw5Hhyyoh6"
   },
   "source": [
    "## Data Normalization\n",
    "Though we can observe that the neural network is learning, the rate is __very slow__ and __learning rate deteriorates very quickly__. <br>\n",
    "This behaviour is due to extreme differences between `max` (255) and `min` (0) values of our dataset. <br>\n",
    "Neural networks are performing __best when dataset ranges from 0 to 1__, or in some cases -1 to 1. <br>\n",
    "Since we can imagine these values as signal strength, very high values, such as 255, are way too overpowering and strengthening non-optimal paths too quickly. <br>\n",
    "Therefore we need to divide our training and testing dataset by 255 to get values ranging from 0 to 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KtcCvFS_yoh6"
   },
   "outputs": [],
   "source": [
    "X_train = X_train/255\n",
    "X_test = X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mcv3HyKjyoh7"
   },
   "source": [
    "## Training on Normalized dataset\n",
    "We will generate a new model and train it on normalized dataset. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "EbhdHrB_yoh7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.1506 - accuracy: 0.9549 - val_loss: 0.0617 - val_accuracy: 0.9812\n",
      "Epoch 2/4\n",
      "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0530 - accuracy: 0.9841 - val_loss: 0.0524 - val_accuracy: 0.9818\n",
      "Epoch 3/4\n",
      "60000/60000 [==============================] - 3s 51us/sample - loss: 0.0350 - accuracy: 0.9893 - val_loss: 0.0442 - val_accuracy: 0.9858\n",
      "Epoch 4/4\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.0242 - accuracy: 0.9925 - val_loss: 0.0443 - val_accuracy: 0.9859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8594171150>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=4, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02HPQHUpyoh9"
   },
   "source": [
    "# Saving the model\n",
    "\n",
    "You can use `model.save(filepath)` to save a Keras model into a single HDF5 file which will contain:\n",
    "\n",
    "- the architecture of the model, allowing to re-create the model\n",
    "- the weights of the model\n",
    "- the training configuration (loss, optimizer)\n",
    "- the state of the optimizer, allowing to resume training exactly where you left off.\n",
    "\n",
    "You can then use `keras.models.load_model(filepath)` to re-instantiate your model. `load_model` will also take care of compiling the model using the saved training configuration.\n",
    "\n",
    "Clicking the folder icon ![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABUAAAAVCAYAAACpF6WWAAAAiUlEQVQ4je3TsQ0DIQyF4Vv17UDtmklo3XsHarODa2qnyikS+HSckiIST/pLvgLJh/9gx0bPmZm31oZ6789QM3MA04hoCT5RZg5RAJ5S8pzztFLKNUpE4eNZROQAvNYao6q69H+q6gCcmTe60RkqItNTjRKRGL060zuZ2Yi+YVVd7hMc0G/tf9AXcKBAsT6KvzEAAAAASUVORK5CYII=) to the left shows the notebook's file browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "YQ2ZQ6fsyoh9"
   },
   "outputs": [],
   "source": [
    "model.save('my_model.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-oSrewn6yoh_"
   },
   "source": [
    "## Predicting custom images\n",
    "You can use model you created to classify any image you want, but it is important that you pre-process the image correctly before inputting it into the model. <br>\n",
    "In following code, we take a random (100th) image from testing dataset and classify it. <br>\n",
    "All inputs to our network have to have same shape structure `(batch_size, width, height, channels)`. <br>\n",
    "Since we want to predict only one image, our batch_size has to be 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "P4WIJoh5yoiA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "(1, 28, 28, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown initializer: GlorotUniform",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8153991f6f2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_image\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'my_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;31m# Classify selected image using our model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/padchest/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mopened_new_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/padchest/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36m_deserialize_model\u001b[0;34m(f, custom_objects, compile)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m     \u001b[0mmodel_weights_group\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/padchest/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    456\u001b[0m                         '`Sequential.from_config(config)`?')\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/padchest/lib/python3.7/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/padchest/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                     custom_objects=dict(list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n\u001b[0;32m--> 145\u001b[0;31m                                         list(custom_objects.items())))\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/padchest/lib/python3.7/site-packages/keras/engine/sequential.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_configs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             layer = layer_module.deserialize(conf,\n\u001b[0;32m--> 300\u001b[0;31m                                              custom_objects=custom_objects)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbuild_input_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/padchest/lib/python3.7/site-packages/keras/layers/__init__.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                                     printable_module_name='layer')\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/padchest/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    145\u001b[0m                                         list(custom_objects.items())))\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mCustomObjectScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# Then `cls` may be a function returning a class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/padchest/lib/python3.7/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m         \"\"\"\n\u001b[0;32m-> 1109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcount_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/padchest/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/padchest/lib/python3.7/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0mkernel_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mbias_constraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             **kwargs)\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/padchest/lib/python3.7/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_initializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_initializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_regularizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/padchest/lib/python3.7/site-packages/keras/initializers.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'config'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/padchest/lib/python3.7/site-packages/keras/initializers.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    501\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m                                     printable_module_name='initializer')\n\u001b[0m\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/padchest/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[0;32m--> 138\u001b[0;31m                                  ': ' + class_name)\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'from_config'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mcustom_objects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_objects\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown initializer: GlorotUniform"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Get image from testing dataset\n",
    "test_image = X_test_orig[100]\n",
    "\n",
    "# Display this image, so we have visual feedback\n",
    "plt.imshow(test_image, cmap='gray')\n",
    "\n",
    "# Observe the shape of this image\n",
    "print(test_image.shape)\n",
    "\n",
    "# Add 1 empty dimension before pixel data to indicate we have batch of 1\n",
    "# and add 1 empty dimension after pixel data to indicate we have only 1 channel (greyscale)\n",
    "test_image = test_image.reshape(1,28,28,1)\n",
    "print(test_image.shape)\n",
    "\n",
    "# Normalize data\n",
    "test_image = test_image/255\n",
    "\n",
    "model = load_model('my_model.h5')\n",
    "# Classify selected image using our model\n",
    "prediction = model.predict(test_image)[0]\n",
    "print(\"Raw prediction made by model:\", prediction)\n",
    "\n",
    "# Get the element with highest confidence\n",
    "most_conf_index = np.argmax(prediction)\n",
    "answer_confidence = prediction[most_conf_index]\n",
    "\n",
    "print(\"Model classified image as\", most_conf_index, \"with\", answer_confidence,\"confidence\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGIIpfC3yoiC"
   },
   "source": [
    "## Tensorflow Playground\n",
    "\n",
    "To better visualise the importance of feature maps and feature extraction you can visit the following website and experiment with the structure of the network, hyperparameters to get instant visual feedback and see how your changes reflect the detection of features.\n",
    "\n",
    "<a href=\"https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.77793&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\">Here\n",
    "\n",
    "\n",
    "![tensorboard](https://raw.githubusercontent.com/RetinalSW/COM3025/master/data/Tensorboard.png)\n",
    "</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.cs.ryerson.ca/~aharley/vis/conv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GFjIj2Ysm-3G"
   },
   "source": [
    "# Challenge\n",
    "# <font color='red'> Task of this Lab </font>\n",
    "- We a bunch of image data about **Afghan Hounds** and **Bedlington** (Two types of dogs)\n",
    "  - Labels:\n",
    "    - 0: an Afghan Hound\n",
    "    - 1: a Bedlington Terrier\n",
    "  - can you create a classifier that predict the which image data belongs to which kinds of dog?\n",
    "  - You need to build a model to classify image into 2 types of dogs\n",
    "  - after you've built the model, you need to assign a predicted value to `y_pred` based on `x_test`\n",
    "  - The predicted reults should have a prediction accuracy > 0.7\n",
    "  - The **greyscale** image data has been processed as 100x100x1 numpy array (for a single image)\n",
    "  - Note:\n",
    "    - You can flatten the image into 1-D array, so you can process it with linear classifier\n",
    "    - You might need to normalize the pixel value of the image to get good performance\n",
    "    - You can use loss related to binary\n",
    "    - Different activation functions might have great impact to the result, please think about why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7Fp-ZV35NnM"
   },
   "source": [
    "### Example of images\n",
    "##### Afghan Hound\n",
    "![Afghan Hound](https://raw.githubusercontent.com/hqsiswiliam/surrey_ai_lab_dataset/master/dogs/dog0_grey.png)\n",
    "##### Bedlington Terrier\n",
    "![Bedlington Terrier](https://raw.githubusercontent.com/hqsiswiliam/surrey_ai_lab_dataset/master/dogs/dog1_grey.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xg3rMWHEn2E9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "# We will help you to install necessary components\n",
    "%%capture\n",
    "!rm *.txt *.pyc > /dev/null\n",
    "!wget http://35.197.245.114:8765/static/requirements.txt\n",
    "!wget http://35.197.245.114:8765/static/challenge.pyc\n",
    "!wget http://35.197.245.114:8765/static/ImagePredictionDogs.pyc\n",
    "!pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "prAkM5kvoFHE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading cache_data/dogs/dogs_train.h5\n",
      "[==================================================]Downloading cache_data/dogs/dogs_test.h5\n",
      "[==================================================]"
     ]
    }
   ],
   "source": [
    "from ImagePredictionDogs import AILabDogsClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "SuqLrc8JoLBm"
   },
   "outputs": [],
   "source": [
    "# Please enter your URN to the URN varibale here, it's important!\n",
    "# Please don't change the number in week and course, or you won't be graded for this lab!\n",
    "\n",
    "URN = 'PELASE ENTER YOUR RUN'\n",
    "week = 2\n",
    "course_module = 'com3025'\n",
    "\n",
    "task = AILabDogsClassification(URN, week, course_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ebLGe5cJo1O4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(267, 100, 100)\n",
      "(67, 100, 100)\n"
     ]
    }
   ],
   "source": [
    "# We've prepared the data for you, you need to train a model based on the data we provides\n",
    "\n",
    "x_train_dog, y_train_dog, x_test_dog = task.get_train_data()\n",
    "print(x_train_dog.shape)\n",
    "print(x_test_dog.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_d = x_train_dog.reshape(267,100, 100,1)\n",
    "x_test_d = x_test_dog.reshape(67,100, 100,1)\n",
    "\n",
    "y_train = to_categorical(y_train_dog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "id": "7rzMbRpB6LEI",
    "outputId": "c8392fec-45ec-4a56-8d3e-dbf9f2cf7ae1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class of this picture is: [0. 1.]\n"
     ]
    }
   ],
   "source": [
    "# You can plot the image if you like\n",
    "from ImagePredictionDogs import show_grey_picture\n",
    "show_grey_picture(x_train_dog[10])\n",
    "print(\"Class of this picture is: {}\".format(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_for_dog():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=3, activation=\"relu\", input_shape=(100,100,1)))\n",
    "    model.add(Conv2D(32, kernel_size=3, activation=\"relu\"))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(2, activation=\"softmax\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "1RUtc_gr6Iy2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 267 samples\n",
      "Epoch 1/3\n",
      "267/267 [==============================] - 1s 3ms/sample - loss: 8.4109 - accuracy: 0.4494\n",
      "Epoch 2/3\n",
      "267/267 [==============================] - 0s 300us/sample - loss: 7.4101 - accuracy: 0.5169\n",
      "Epoch 3/3\n",
      "267/267 [==============================] - 0s 291us/sample - loss: 7.4101 - accuracy: 0.5169\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f84580be610>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "## 1) reshape input data\n",
    "## 2) change the number nodes of dense layer\n",
    "## 3) change input shape in the model\n",
    "## 4) change loss metric\n",
    "\n",
    "model = create_model_for_dog()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train_d, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "hxLQ5V38o15J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Then, you will use the trained model to get y_pred, and this will be sent to us\n",
    "\n",
    "y_pred = model.predict(x_test_d) # make prediction here\n",
    "# Please use the following code to evaluate your result,  this is a necessary step, best regards!\n",
    "# Make sure the input of the evaluation is 1D array!\n",
    "task.evaluate(y_pred.flatten().tolist(), model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2-CNN-1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "deep_learning_modules",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "nteract": {
   "version": "0.21.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "ebe30650359257499db5b8257f9b03a9756a82ad348cb7c2e2c49f1535a1520b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
